{
   "cells": [
      {
         "cell_type": "code",
         "execution_count": 1,
         "id": "4cac9c10",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
                  "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
               ]
            }
         ],
         "source": [
            "import unsloth\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "2dbc484a",
         "metadata": {},
         "outputs": [],
         "source": [
            "from unsloth import FastLanguageModel, to_sharegpt\n",
            "from datasets import load_dataset, Dataset\n",
            "import torch\n",
            "from trl import SFTTrainer\n",
            "from transformers.training_args import TrainingArguments\n",
            "from unsloth import is_bfloat16_supported\n",
            "import pandas as pd"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "fb5cf9d9",
         "metadata": {},
         "outputs": [],
         "source": [
            "# Define parameters\n",
            "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
            "max_seq_length = 2048\n",
            "dtype = None\n",
            "load_in_4bit = True\n",
            "token = \"your HF_Token\"  #replace by the token created from hugging face\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 5,
         "id": "b600a300",
         "metadata": {},
         "outputs": [],
         "source": [
            "# Declare model\n",
            "model, tokenizer = FastLanguageModel.from_pretrained(\n",
            "    model_name= model_name,\n",
            "    max_seq_length= max_seq_length,\n",
            "    dtype= dtype,\n",
            "    load_in_4bit= load_in_4bit,\n",
            "    token= token,\n",
            ")\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "912c61b8",
         "metadata": {},
         "outputs": [],
         "source": [
            "from huggingface_hub import HfFolder\n",
            "from datasets import config\n",
            "\n",
            "# ƒê·ªãa ch·ªâ cache cho c√°c t·ªáp t·ª´ Hugging Face Hub (m√¥ h√¨nh, tokenizer)\n",
            "cache_dir = config.HF_DATASETS_CACHE\n",
            "\n",
            "print(cache_dir)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 6,
         "id": "ab5a8745",
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "application/vnd.jupyter.widget-view+json": {
                     "model_id": "4a536b77a542413fa1b842a66df3a926",
                     "version_major": 2,
                     "version_minor": 0
                  },
                  "text/plain": [
                     "Resolving data files:   0%|          | 0/89 [00:00<?, ?it/s]"
                  ]
               },
               "metadata": {},
               "output_type": "display_data"
            },
            {
               "data": {
                  "application/vnd.jupyter.widget-view+json": {
                     "model_id": "194dcf3e74f649e5a7a9b2ef31f0215d",
                     "version_major": 2,
                     "version_minor": 0
                  },
                  "text/plain": [
                     "Loading dataset shards:   0%|          | 0/87 [00:00<?, ?it/s]"
                  ]
               },
               "metadata": {},
               "output_type": "display_data"
            }
         ],
         "source": [
            "# Load dataset\n",
            "dataset_eng_qa = load_dataset(\"lavita/medical-qa-datasets\", \"all-processed\", split = \"train\")\n",
            "dataset_eng_qa_2 = load_dataset(\"eashuu/medical_qa\", split = \"train\")\n",
            "dataset_viet_qa = load_dataset(\"hungnm/vietnamese-medical-qa\", split=\"train\")\n",
            "dataset_viet_diagnosis = load_dataset(\"PB3002/ViMedical_Disease\", split=\"train\")\n",
            "dataset_vipubmed = load_dataset(\"VietAI/vi_pubmed\", split = \"pubmed22\")\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 7,
         "id": "9dbf599a",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "['instruction', 'input', 'output', '__index_level_0__']\n",
                  "['instruction', 'input', 'output']\n",
                  "['answer', 'question']\n",
                  "['Disease', 'Question']\n",
                  "['en', 'vi']\n"
               ]
            }
         ],
         "source": [
            "print(dataset_eng_qa.column_names)\n",
            "print(dataset_eng_qa_2.column_names)\n",
            "print(dataset_viet_qa.column_names)\n",
            "print(dataset_viet_diagnosis.column_names)\n",
            "print(dataset_vipubmed.column_names)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 8,
         "id": "bfd404e3",
         "metadata": {},
         "outputs": [],
         "source": [
            "# Data preprocessing\n",
            "def format_eng_qa(example):\n",
            "    answer = example['output']\n",
            "    output = answer.replace(\"chatbot\", \"telemedicine agent system\")\n",
            "    output = output.replace(\"Chatbot\", \"Telemedicine agent system\")\n",
            "    return {\n",
            "        'instruction': example['instruction'],\n",
            "        'input': example['input'],\n",
            "        'output': output\n",
            "    }\n",
            "dataset_eng_qa = dataset_eng_qa.map(format_eng_qa)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 9,
         "id": "fae53e79",
         "metadata": {},
         "outputs": [],
         "source": [
            "def format_viet_qa(example):\n",
            "    # Return dict directly, not wrapped in a list\n",
            "    return {\n",
            "        \"instruction\": \"ƒë·ªçc th√¥ng tin sau v√† gi·∫£i ƒë√°p c√¢u h·ªèi c·ªßa b·ªánh nh√¢n\",\n",
            "        \"input\": example[\"question\"],\n",
            "        \"output\": example[\"answer\"],\n",
            "    }\n",
            "dataset_viet_qa = dataset_viet_qa.map(format_viet_qa)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 10,
         "id": "fdfb1d74",
         "metadata": {},
         "outputs": [],
         "source": [
            "def format_diagnosis(example):\n",
            "    disease = example[\"Disease\"]\n",
            "    answer = (\n",
            "        f\"D·ª±a tr√™n tri·ªáu ch·ª©ng b·∫°n m√¥ t·∫£, c√≥ th·ªÉ b·∫°n ƒëang m·∫Øc b·ªánh {disease}. \"\n",
            "        \"Tuy nhi√™n, ƒë√¢y ch·ªâ l√† ƒë√°nh gi√° s∆° b·ªô. B·∫°n n√™n ƒëi kh√°m b√°c sƒ© ƒë·ªÉ ƒë∆∞·ª£c ch·∫©n ƒëo√°n ch√≠nh x√°c.\"\n",
            "    )\n",
            "    return {\n",
            "        \"instruction\": \"D·ª±a v√†o nh·ªØng tri·ªáu ch·ª©ng m√† b·ªánh nh√¢n m√¥ t·∫£, h√£y ƒë∆∞a ra ch·∫©n ƒëo√°n b·ªánh.\",\n",
            "        \"input\": example[\"Question\"],\n",
            "        \"output\": answer,\n",
            "    }\n",
            "\n",
            "def format_vipubmed(example):\n",
            "    instruction = \"H√£y ƒë·ªçc vƒÉn b·∫£n ti·∫øng anh do ng∆∞·ªùi d√πng cung c·∫•p v√† d·ªãch sang ti·∫øng vi·ªát.\"\n",
            "    en_text = example['en']\n",
            "    input = f\"B·∫°n h√£y gi√∫p t√¥i d·ªãch vƒÉn b·∫£n sau ƒë√¢y sang ti·∫øng Vi·ªát: {en_text}\"\n",
            "    output = f\"VƒÉn b·∫£n sau khi ƒë∆∞·ª£c d·ªãch l√†:\\n\\t{example['vi']}\"\n",
            "    return {\n",
            "        'instruction': instruction,\n",
            "        'input': input,\n",
            "        'output': output,\n",
            "    }\n",
            "\n",
            "dataset_eng_qa = dataset_eng_qa.map(format_eng_qa)\n",
            "dataset_viet_qa = dataset_viet_qa.map(format_viet_qa)\n",
            "dataset_viet_diagnosis = dataset_viet_diagnosis.map(format_diagnosis)\n",
            "dataset_vipubmed = dataset_vipubmed.map(format_vipubmed)\n",
            "\n",
            "dataset = pd.concat([\n",
            "    pd.DataFrame(dataset_eng_qa),\n",
            "    pd.DataFrame(dataset_eng_qa_2),\n",
            "    pd.DataFrame(dataset_viet_qa),\n",
            "    pd.DataFrame(dataset_viet_diagnosis),\n",
            "    pd.DataFrame(dataset_vipubmed),\n",
            "], ignore_index=True)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 12,
         "id": "209d43b6",
         "metadata": {},
         "outputs": [],
         "source": [
            "dataset = pd.concat([\n",
            "    pd.DataFrame(dataset_eng_qa),\n",
            "    pd.DataFrame(dataset_eng_qa_2[:5000]),\n",
            "    pd.DataFrame(dataset_viet_qa),\n",
            "], ignore_index= True)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 13,
         "id": "b6a56b56",
         "metadata": {},
         "outputs": [],
         "source": [
            "dataset=pd.concat([dataset,pd.DataFrame(dataset_viet_diagnosis),], ignore_index= True)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 14,
         "id": "c084bb63",
         "metadata": {},
         "outputs": [],
         "source": [
            "dataset = pd.concat([dataset, pd.DataFrame(dataset_vipubmed[:10000]),], ignore_index= True)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 1,
         "id": "328b2afb",
         "metadata": {},
         "outputs": [
            {
               "ename": "NameError",
               "evalue": "name 'to_sharegpt' is not defined",
               "output_type": "error",
               "traceback": [
                  "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                  "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
                  "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mto_sharegpt\u001b[49m(\n\u001b[1;32m      2\u001b[0m     Dataset\u001b[38;5;241m.\u001b[39mfrom_pandas(dataset),\n\u001b[1;32m      3\u001b[0m     merged_prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{instruction}\u001b[39;00m\u001b[38;5;124m[[\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mYour input is:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{input}\u001b[39;00m\u001b[38;5;124m]]\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m     output_column_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     conversation_extension\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m      6\u001b[0m )\n",
                  "\u001b[0;31mNameError\u001b[0m: name 'to_sharegpt' is not defined"
               ]
            }
         ],
         "source": [
            "dataset = to_sharegpt(\n",
            "    Dataset.from_pandas(dataset),\n",
            "    merged_prompt=\"{instruction}[[\\nYour input is:\\n{input}]]\",\n",
            "    output_column_name=\"output\",\n",
            "    conversation_extension=3,\n",
            ")\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "44834721",
         "metadata": {},
         "outputs": [],
         "source": [
            "# Define chat template\n",
            "chat_template =\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "{SYSTEM}<|eot_id|><start_header_id|>user<|end_header_id|>\n",
            "{INPUT}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "{OUTPUT}<|eot_id|>\n",
            "\"\"\"\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "7816443f",
         "metadata": {},
         "outputs": [],
         "source": [
            "# Create fine-tune model\n",
            "model = FastLanguageModel.get_peft_model(\n",
            "    model,\n",
            "    r = 16,\n",
            "    target_modules= [\"q_proj\", \"k_proj\", \"v_proj\", \n",
            "                     \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
            "    lora_alpha= 16,\n",
            "    lora_dropout= 0,\n",
            "    bias = \"none\",\n",
            "    use_gradient_checkpointing= \"unsloth\",\n",
            "    random_state= 3407,\n",
            "    use_rslora= False,\n",
            "    loftq_config= None\n",
            ")\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "165af75f",
         "metadata": {},
         "outputs": [],
         "source": [
            "def formatting_func(batch):\n",
            "    # batch is a dict: {\"conversations\": [ [...], [...], ... ]}\n",
            "    formatted_texts = []\n",
            "\n",
            "    system_prompt = \"You are a helpful medical assistant. Answer based on the patient's description.\"\n",
            "\n",
            "    for conversation in batch[\"conversations\"]:  # Each `conversation` is a list of {'from': ..., 'value': ...}\n",
            "        text = \"<|begin_of_text|>\"\n",
            "\n",
            "        for msg in conversation:\n",
            "            if not isinstance(msg, dict):\n",
            "                continue\n",
            "            if \"from\" not in msg or \"value\" not in msg:\n",
            "                continue\n",
            "\n",
            "            role = msg[\"from\"]\n",
            "            content = msg[\"value\"].strip()\n",
            "\n",
            "            if role == \"human\":\n",
            "                if \"If you are a doctor\" in content and \"Your input is:\" in content:\n",
            "                    try:\n",
            "                        user_input = content.split(\"Your input is:\\n\", 1)[1]\n",
            "                    except IndexError:\n",
            "                        user_input = content\n",
            "                    text += f\"<|start_header_id|>system<|end_header_id|>\\n{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n{user_input}<|eot_id|>\"\n",
            "                else:\n",
            "                    text += f\"<|start_header_id|>user<|end_header_id|>\\n{content}<|eot_id|>\"\n",
            "            elif role == \"gpt\":\n",
            "                text += f\"<|start_header_id|>assistant<|end_header_id|>\\n{content}<|eot_id|>\"\n",
            "\n",
            "        formatted_texts.append(text)\n",
            "\n",
            "    return formatted_texts  # List of strings, one per example\n",
            "\n",
            "trainer = SFTTrainer(\n",
            "    model = model,\n",
            "    processing_class= tokenizer,\n",
            "    train_dataset= dataset,\n",
            "    peft_config= model.peft_config,\n",
            "    formatting_func = formatting_func,\n",
            "    args= TrainingArguments(\n",
            "        output_dir= \"./output\",\n",
            "        per_device_train_batch_size= 2,\n",
            "        gradient_accumulation_steps= 4,\n",
            "        warmup_steps= 5,\n",
            "        max_steps= 60,\n",
            "        num_train_epochs= 1,\n",
            "        learning_rate= 2e-4,\n",
            "        fp16= not is_bfloat16_supported(),\n",
            "        bf16= is_bfloat16_supported(),\n",
            "        logging_steps= 1,\n",
            "        optim=\"adamw_8bit\",\n",
            "        weight_decay= 0.01,\n",
            "        lr_scheduler_type= \"linear\",\n",
            "        seed= 3407,\n",
            "    )\n",
            ")\n",
            "trainer.train()\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "18b430ed",
         "metadata": {},
         "outputs": [],
         "source": [
            "# Save model\n",
            "model.save_pretrained(\"../models/fine_tuned_model\")\n",
            "tokenizer.save_pretrained(\"../models/fine_tuned_model\")\n"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": ".venv",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.10.12"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 5
}
